{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from model import AzureOpenAIModel\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wiki data\n",
    "docs = WikipediaLoader(query=\"KP Sharma Oli\", load_max_docs=2).load()\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# # Run the hugging face embeddings on the splits\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"all-MiniLM-L6-v2\", cache_folder=\"./embedding_model\"\n",
    "# )\n",
    "\n",
    "# OpenAI embedding\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_EMBEDDING_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_EMBEDDING_ENDPOINT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store from the embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits, embedding=embeddings, persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination due to outdated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = AzureOpenAIModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KP Sharma Oli was last elected as the Prime Minister of Nepal on February 15, 2018.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "You are a helpful assistant in answering about Nepal's political scene. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "human_prompt = \"\"\"\\\n",
    "Answer the following question: {question}\n",
    "\"\"\"\n",
    "question = \"When was the last time KP Sharma Oli was elected as the Prime Minister of Nepal?\"\n",
    "input_message = {\"question\": question}\n",
    "\n",
    "gpt_model.run_model(\n",
    "    system_prompt=system_prompt,\n",
    "    human_prompt=human_prompt,\n",
    "    input_message=input_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grounded Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The last time KP Sharma Oli was elected as the Prime Minister of Nepal was on 15 July 2024.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_prompt = \"\"\"\\\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\n",
    "\"\"\"\n",
    "\n",
    "question = \"When was the last time KP Sharma Oli was elected as the Prime Minister of Nepal\"\n",
    "gpt_model.run_rag_model(\n",
    "    retriever=retriever,\n",
    "    question=question,\n",
    "    human_prompt=human_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
